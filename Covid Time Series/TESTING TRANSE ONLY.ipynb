{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-RBf2F-Ffi3"
      },
      "source": [
        "**Simple Schemes for Knowledge Graph Embedding**\n",
        "\n",
        "This colab presents code for training knowledge graph embeddings on the FB15k-237 knowledge base using two KG embedding techniques (rotatE and transE) and evaluates the metrics on MRR and Hits@K. This colab can be run to save entitiy and relation embeddings at every 10 epochs.\n",
        "\n",
        "Included in the colab is also code to preprocess the data and display the dimensional embedings to illustrate the geometrical properties of the embedding schemas (Visualization Tools and MID to Entity Name Mapping sections). However, this portion of the code was run on a GCP VM and does not run out of the box on colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ndxAI0r36m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DBX9X7Yr4E3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1m6eDG4YWuC"
      },
      "source": [
        "**Custom DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXanUwiTixO1",
        "outputId": "b5c6080d-d306-43c4-855d-ab7554be3d45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1880fc9e550>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Allow for reproducible results\n",
        "np.random.seed(234)\n",
        "torch.manual_seed(234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wttwmJH-YVO8"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Dataset):\n",
        "\n",
        "    def __init__(self, triples, num_entities, num_negative_samples, all_triples=None, data_type=\"train\"):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.triples = triples\n",
        "        self.num_entities = num_entities\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        self.all_triples = all_triples\n",
        "        self.data_type = data_type\n",
        "        self.len = len(triples)\n",
        "        self.true_head_relation, self.true_relation_tail = self._get_true_head_tail_lists()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        positive_sample = self.triples[idx]\n",
        "        head, relation, tail = positive_sample\n",
        "\n",
        "        positive_sample = torch.LongTensor(positive_sample)\n",
        "\n",
        "        if self.data_type == \"train\":\n",
        "            true_heads = self.true_relation_tail[(relation, tail)]\n",
        "            true_tails = self.true_head_relation[(head, relation)]\n",
        "\n",
        "            # A number of negative sampling methods was tried - including\n",
        "            # taking a set difference before random sampling, randomly sampling\n",
        "            # each corrupted head, etc but the method implemented in the paper\n",
        "            # ended up being the fastest\n",
        "            corrupted_heads = []\n",
        "            corrupted = np.random.randint(self.num_entities, size=self.num_negative_samples*2)\n",
        "            while len(corrupted_heads) < self.num_negative_samples:\n",
        "                mask = np.in1d(corrupted, true_heads, assume_unique=True, invert=True)\n",
        "                corrupted = corrupted[mask]\n",
        "                corrupted_heads.extend(corrupted)\n",
        "            corrupted_heads = corrupted_heads[:self.num_negative_samples]\n",
        "            corrupted_heads = torch.LongTensor(corrupted_heads)\n",
        "\n",
        "            corrupted_tails = []\n",
        "            corrupted = np.random.randint(self.num_entities, size=self.num_negative_samples*2)\n",
        "            while len(corrupted_tails) < self.num_negative_samples:\n",
        "                mask = np.in1d(corrupted, true_tails, assume_unique=True, invert=True)\n",
        "                corrupted = corrupted[mask]\n",
        "                corrupted_tails.extend(corrupted)\n",
        "            corrupted_tails = corrupted_tails[:self.num_negative_samples]\n",
        "            corrupted_tails = torch.LongTensor(corrupted_tails)\n",
        "\n",
        "            filter_bias = torch.LongTensor([0] * len(positive_sample))\n",
        "        else:\n",
        "            # We cannot empirically say that one head is better than another for a valid\n",
        "            # (head, relation, tail) triplet. Ex. (Bob, friend, Joe), (Jack, friend, Joe).\n",
        "            # In this case, we replace the alternate triplet with the current true head\n",
        "            # and add a filter bias of -1 to push it down in the rankings so ideally prevent it\n",
        "            # from showing up in our HITS@K and MRR metrics\n",
        "            corrupted_heads = [(0, test_head) if (test_head, relation, tail) not in self.all_triples\n",
        "                    else (-1, head) for test_head in range(self.num_entities)]\n",
        "            corrupted_heads[head] = (0, head)\n",
        "            corrupted_heads = torch.LongTensor(corrupted_heads)\n",
        "\n",
        "            corrupted_tails = [(0, test_tail) if (head, relation, test_tail) not in self.all_triples\n",
        "                    else (-1, tail) for test_tail in range(self.num_entities)]\n",
        "            corrupted_tails[tail] = (0, tail)\n",
        "            corrupted_tails = torch.LongTensor(corrupted_tails)\n",
        "\n",
        "            filter_bias = (corrupted_heads[:, 0], corrupted_tails[:, 0])\n",
        "            corrupted_heads = corrupted_heads[:, 1]\n",
        "            corrupted_tails = corrupted_tails[:, 1]\n",
        "\n",
        "        return positive_sample, corrupted_heads, corrupted_tails, filter_bias\n",
        "\n",
        "    # We need to be able to get a list of true heads and tails\n",
        "    # quickly during negative sampling\n",
        "    def _get_true_head_tail_lists(self):\n",
        "        true_head_relation = defaultdict(set)\n",
        "        true_relation_tail = defaultdict(set)\n",
        "        for triplet in self.triples:\n",
        "            head, relation, tail = triplet\n",
        "            true_head_relation[(head, relation)].add(tail)\n",
        "            true_relation_tail[(relation, tail)].add(head)\n",
        "        return true_head_relation, true_relation_tail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F3brjpOUig8L"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(triples, num_entities, num_negative_samples, batch_size,\n",
        "        all_triples=None, data_type=\"train\", num_workers=0):\n",
        "    return DataLoader(\n",
        "        DataGenerator(triples, num_entities, num_negative_samples, all_triples=all_triples, data_type=data_type),\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True,\n",
        "        num_workers = num_workers\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWCD8YXO-ILW"
      },
      "source": [
        "**RotatE**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bgqqbg6mYtpu"
      },
      "outputs": [],
      "source": [
        "def rotatE(head, relation, tail, embedding_range, sample_type, margin):\n",
        "    # We used 2 x hidden_dim so that we can split them into real and imaginary\n",
        "    # components here\n",
        "    head_real, head_imag = torch.chunk(head, 2, dim=2)\n",
        "    tail_real, tail_imag = torch.chunk(tail, 2, dim=2)\n",
        "\n",
        "    # This evenly distributes the relation between [-pi, pi]\n",
        "    norm_relation = relation / (embedding_range / math.pi)\n",
        "\n",
        "    relation_real = torch.cos(norm_relation)\n",
        "    relation_imag = torch.sin(norm_relation)\n",
        "\n",
        "    real_dist = (head_real * relation_real - head_imag * relation_imag) - tail_real\n",
        "    imag_dist = (head_real * relation_imag + head_imag * relation_real) - tail_imag\n",
        "\n",
        "    # Each dimension represents its own rotation in imaginary space.\n",
        "    # Take the Frobenius norm to compute the score and sum across all\n",
        "    # dimensions\n",
        "    total_dist = torch.stack([real_dist, imag_dist], dim=0)\n",
        "    total_dist = torch.linalg.norm(total_dist, dim=0).sum(dim=2)\n",
        "\n",
        "    # If something is close enough, we don't want to penalize it\n",
        "    margin_adjusted_dist = margin - total_dist\n",
        "    return margin_adjusted_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVqweJ64_2W0"
      },
      "source": [
        "**TransE**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7ouvCRa_9maG"
      },
      "outputs": [],
      "source": [
        "def transE(head, relation, tail, sample_type, margin):\n",
        "  dist = head + relation - tail\n",
        "  score = margin - torch.linalg.norm(dist, ord=1, dim=2)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBETcaB6hEhO"
      },
      "source": [
        "**Generic Knowledge Graph Embedding Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yuJX1zs2iMIG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8ngQUdDkAGEO"
      },
      "outputs": [],
      "source": [
        "class KGEmbedding(nn.Module):\n",
        "  def __init__(self, num_entities, num_relations, hidden_dim, margin, model):\n",
        "    super(KGEmbedding, self).__init__()\n",
        "\n",
        "    self.num_entities = num_entities\n",
        "    self.num_relations = num_relations\n",
        "    # Entity embed needs to have out dim 2 x hidden_dim for rotatE\n",
        "    # b/c each dimension needs to have a real and imaginary component\n",
        "    self.entity_dim = 2 * hidden_dim if model == 'rotatE' else hidden_dim\n",
        "    self.relation_dim = hidden_dim\n",
        "    self.margin = margin\n",
        "    self.model = model\n",
        "    self.epsilon = 2.0\n",
        "\n",
        "    # In the paper (default):\n",
        "    # margin = 12.0\n",
        "    # hidden_dim = 500\n",
        "    # epsilon = 2.0\n",
        "    # embedding_range = (margin + epsilon) / hidden_dim = 0.028\n",
        "    # This means that all entity embeddings and relation embeddings\n",
        "    # are initialized with value between -0.028 and 0.028\n",
        "\n",
        "    # Initialization of the embedding range to be close to 0 helps prevent\n",
        "    # crazy initializations. We divide by hidden_dim to reduce variance in\n",
        "    # per-parameter initialization as we increase total number of dimensions\n",
        "    # co consider\n",
        "    self.embedding_range = (self.margin + self.epsilon) / hidden_dim\n",
        "\n",
        "    self.entity_embed = nn.Parameter(torch.zeros(self.num_entities, self.entity_dim,\n",
        "                                    requires_grad=True))\n",
        "    nn.init.uniform_(\n",
        "        self.entity_embed,\n",
        "        a=-self.embedding_range,\n",
        "        b=self.embedding_range\n",
        "    )\n",
        "\n",
        "    # Relation entity can only affect the phase, not the modulus of the\n",
        "    # entity embedding. The modulus is fixed to be |r_i| = 1.\n",
        "    self.relation_embed = nn.Parameter(torch.zeros(self.num_relations, self.relation_dim,\n",
        "                                      requires_grad=True))\n",
        "    nn.init.uniform_(\n",
        "        self.relation_embed,\n",
        "        a=-self.embedding_range,\n",
        "        b=self.embedding_range\n",
        "    )\n",
        "\n",
        "  def forward(self, sample, sample_type):\n",
        "    # For each positive example, the paper has 128 negative examples using\n",
        "    # the same head + relation or relation + tail but with a corrupted entity\n",
        "\n",
        "    # Sample can be a positive example, a negative tail example, or a\n",
        "    # negative head example. Each needs to be processed slightly\n",
        "    # differently before being passed to the respective model function\n",
        "\n",
        "    if sample_type == 'positive':\n",
        "      # sample = Tensor([batch_size, 3]) where the\n",
        "      # 3 represents head, relation, tail\n",
        "      head = torch.index_select(\n",
        "          self.entity_embed,\n",
        "          dim=0,\n",
        "          index=sample[:,0]\n",
        "      ).unsqueeze(1)\n",
        "\n",
        "      relation = torch.index_select(\n",
        "          self.relation_embed,\n",
        "          dim=0,\n",
        "          index=sample[:,1]\n",
        "      ).unsqueeze(1)\n",
        "\n",
        "      tail = torch.index_select(\n",
        "          self.entity_embed,\n",
        "          dim=0,\n",
        "          index=sample[:,2]\n",
        "      ).unsqueeze(1)\n",
        "\n",
        "      # We call .unsqueeze(1) on this data so that\n",
        "      # the output will be of dimension [batch_size, 1, 2 * self.hidden_dim]\n",
        "      # so that the num dimensions match the negative examples that will be of\n",
        "      # dimension [batch_size, num_neg_samples, 2 * self.hiddenIdim]\n",
        "\n",
        "    elif sample_type == 'negative-head':\n",
        "      # positive_tuple is torch.Tensor([batch_size, 3])\n",
        "      # negative_head_entities is torch.Tensor([batch_size, num_neg_samples]) and\n",
        "      # needs to be used in conjunction with positive_tuple to derive the 128\n",
        "      # negative examples for each positive example\n",
        "      positive_tuple, negative_head_entities = sample\n",
        "      batch_size, num_neg_samples = negative_head_entities.shape\n",
        "\n",
        "      head = torch.index_select(\n",
        "          self.entity_embed,\n",
        "          dim=0,\n",
        "          index=negative_head_entities.view(-1)\n",
        "      ).reshape(batch_size, num_neg_samples, self.entity_dim)\n",
        "\n",
        "      relation = torch.index_select(\n",
        "          self.relation_embed,\n",
        "          dim=0,\n",
        "          index=positive_tuple[:,1]\n",
        "      ).unsqueeze(1)\n",
        "\n",
        "      tail = torch.index_select(\n",
        "          self.entity_embed,\n",
        "          dim=0,\n",
        "          index=positive_tuple[:,2]\n",
        "      ).unsqueeze(1)\n",
        "\n",
        "\n",
        "    elif sample_type == 'negative-tail':\n",
        "      # same as \"negative-head\" except this time\n",
        "      # the tail needs to be adjusted\n",
        "      positive_tuple, negative_tail_entities = sample\n",
        "      batch_size, num_neg_samples = negative_tail_entities.shape\n",
        "\n",
        "      head = torch.index_select(\n",
        "          self.entity_embed,\n",
        "          dim=0,\n",
        "          index=positive_tuple[:,0]\n",
        "      ).unsqueeze(1)\n",
        "\n",
        "      relation = torch.index_select(\n",
        "          self.relation_embed,\n",
        "          dim=0,\n",
        "          index=positive_tuple[:,1]\n",
        "      ).unsqueeze(1)\n",
        "\n",
        "      tail = torch.index_select(\n",
        "          self.entity_embed,\n",
        "          dim=0,\n",
        "          index=negative_tail_entities.view(-1)\n",
        "      ).reshape(batch_size, num_neg_samples, self.entity_dim)\n",
        "\n",
        "    if self.model == 'rotatE':\n",
        "      return rotatE(head, relation, tail, self.embedding_range, sample_type, self.margin)\n",
        "    elif self.model == 'transE':\n",
        "      return transE(head, relation, tail, sample_type, self.margin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDxv2egBh-ZN"
      },
      "source": [
        "**Run Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c7AHu_xaiPST"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import trange\n",
        "\n",
        "DATA_DIR = \"transDatacopy/\"\n",
        "MODEL_DIR = \"modelscopy/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "joUtuvBwlmvs"
      },
      "outputs": [],
      "source": [
        "# Read the entities and relations dictionary files\n",
        "def load_dict(file_path):\n",
        "    loaded_dict = dict()\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            uid, val = line.strip().split('\\t')\n",
        "            loaded_dict[val] = int(uid)\n",
        "    return loaded_dict\n",
        "\n",
        "# Read the KG triples\n",
        "def load_triples(file_path, entity2id, relation2id):\n",
        "    triples = list()\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            head, relation, tail = line.strip().split('\\t')\n",
        "            triples.append((entity2id[head], relation2id[relation], entity2id[tail]))\n",
        "    return triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F-zfw1_OloaW"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, epoch):\n",
        "    # Check if MODEL_DIR exists and create if it doesn't\n",
        "    if not os.path.exists(MODEL_DIR):\n",
        "        os.makedirs(MODEL_DIR)\n",
        "\n",
        "    torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict()\n",
        "        },\n",
        "        os.path.join(MODEL_DIR, f'checkpoint_{epoch}')\n",
        "    )\n",
        "\n",
        "    entity_embedding = model.entity_embed.detach().cpu().numpy()\n",
        "    np.save(os.path.join(MODEL_DIR, f'entity_embedding_{epoch}'), entity_embedding)\n",
        "\n",
        "    relation_embedding = model.relation_embed.detach().cpu().numpy()\n",
        "    np.save(os.path.join(MODEL_DIR, f'relation_embedding_{epoch}'), relation_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RbKBneech9-6"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data, num_entities, _num_negative_samples, _batch_size, all_data, data_type):\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = get_data_loader(data, num_entities, _num_negative_samples, _batch_size, all_triples=all_data, data_type=data_type)\n",
        "\n",
        "    final_metrics = defaultdict(float)\n",
        "    dataset_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (positive_sample, corrupted_heads, corrupted_tails, filter_bias) in enumerate(dataloader):\n",
        "            head_bias, tail_bias = filter_bias\n",
        "            if torch.cuda.is_available():\n",
        "                positive_sample = positive_sample.cuda()\n",
        "                corrupted_heads = corrupted_heads.cuda()\n",
        "                corrupted_tails = corrupted_tails.cuda()\n",
        "                head_bias = head_bias.cuda()\n",
        "                tail_bias = tail_bias.cuda()\n",
        "\n",
        "            # When we run eval, this list of \"corrupted\" values does contain one\n",
        "            # (or more*) true positive values. We have to include the true triplet to\n",
        "            # be able to run eval metrics like MRR and Hits@K\n",
        "            # *If there is more than one true positive value, the others are weighted\n",
        "            # negatively such that they appear lower in the ranking and will be unlikely\n",
        "            # to skew the metrics. This is seen in the head_bias and tail_bias below\n",
        "            corrupted_head_dist = model((positive_sample, corrupted_heads), 'negative-head') + head_bias\n",
        "            corrupted_tail_dist = model((positive_sample, corrupted_tails), 'negative-tail') + tail_bias\n",
        "\n",
        "            # We sort by descending, b/c margin in RotatE sets large distances to be negative\n",
        "            head_arg_order = torch.argsort(corrupted_head_dist, dim=1, descending=True)\n",
        "            tail_arg_order = torch.argsort(corrupted_tail_dist, dim=1, descending=True)\n",
        "\n",
        "            true_head = positive_sample[:, 0]\n",
        "            true_tail = positive_sample[:, 2]\n",
        "\n",
        "            for ind in range(len(true_head)):\n",
        "                # Pytorch way to evaluate rank of item in list\n",
        "                true_head_rank = (head_arg_order[ind, :] == true_head[ind]).nonzero()\n",
        "                true_head_rank = true_head_rank.item() + 1\n",
        "\n",
        "                true_tail_rank = (tail_arg_order[ind, :] == true_tail[ind]).nonzero()\n",
        "                true_tail_rank = true_tail_rank.item() + 1\n",
        "\n",
        "                dataset_metrics.append({\n",
        "                    'MRR': 1.0 / true_head_rank,\n",
        "                    'MR': float(true_head_rank),\n",
        "                    'HITS@1': 1.0 if true_head_rank <= 1 else 0.0,\n",
        "                    'HITS@3': 1.0 if true_head_rank <= 3 else 0.0,\n",
        "                    'HITS@10': 1.0 if true_head_rank <= 10 else 0.0\n",
        "                })\n",
        "                dataset_metrics.append({\n",
        "                    'MRR': 1.0 / true_tail_rank,\n",
        "                    'MR': float(true_tail_rank),\n",
        "                    'HITS@1': 1.0 if true_tail_rank <= 1 else 0.0,\n",
        "                    'HITS@3': 1.0 if true_tail_rank <= 3 else 0.0,\n",
        "                    'HITS@10': 1.0 if true_tail_rank <= 10 else 0.0\n",
        "                })\n",
        "        for metric in dataset_metrics:\n",
        "            for key, val in metric.items():\n",
        "                final_metrics[key] += val\n",
        "        for key, val in final_metrics.items():\n",
        "            final_metrics[key] = val / len(dataset_metrics)\n",
        "\n",
        "        print(f\"{data_type} metrics: {final_metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AhKMflBVlrCV"
      },
      "outputs": [],
      "source": [
        "# For training our actual model we ran this on a GCP VM.\n",
        "# The same code will run on Colab but will likely take much longer.\n",
        "\n",
        "def main():\n",
        "    # List of hyper parametrs that we manually tuned using\n",
        "    # the suggestions of the rotatE paper as a baseline. Training\n",
        "    # and modeling was one on a GCP VM rather than in Colab\n",
        "    _num_negative_samples = 128\n",
        "    _batch_size = 1024\n",
        "    _test_batch_size = 16\n",
        "    _lr = 0.0001\n",
        "    _hidden_dim = 2\n",
        "    _margin = 12\n",
        "    _num_epochs = 40\n",
        "    _weight_decay = 5e-5\n",
        "    _cuda = torch.cuda.is_available()\n",
        "\n",
        "    # FB2k-237 data is presented in terms of \"mids\" (Ex. /m/23sdf) that\n",
        "    # represent an entity. To train embeddings for these entities, we need to\n",
        "    # assign each one an index to easily model in a neural network. These\n",
        "    # maps are used to relation entities and relations to indices\n",
        "    entity2id = load_dict(os.path.join(DATA_DIR, 'entities.dict'))\n",
        "    relation2id = load_dict(os.path.join(DATA_DIR, 'relations.dict'))\n",
        "    num_entities = len(entity2id)\n",
        "    num_relations = len(relation2id)\n",
        "\n",
        "    train_data = load_triples(os.path.join(DATA_DIR, 'trainReal.csv'), entity2id, relation2id)\n",
        "    eval_data = load_triples(os.path.join(DATA_DIR, 'valid.csv'), entity2id, relation2id)\n",
        "    test_data = load_triples(os.path.join(DATA_DIR, 'test.csv'), entity2id, relation2id)\n",
        "\n",
        "    # Here we compile a list of all triples for the sake of negative sampling later on.\n",
        "    # We don't want to accidentally consider a true triple as a negative sample\n",
        "    all_data = [train_data, eval_data, test_data]\n",
        "\n",
        "    # Get the train dataloader\n",
        "    dataloader = get_data_loader(train_data, num_entities, _num_negative_samples, _batch_size, data_type=\"train\")\n",
        "\n",
        "    # Here we initialize the model to use the rotatE embedding loss paradigm. When\n",
        "    # we trained our transE model, we flipped the final parameter to \"transE\"\n",
        "    model = KGEmbedding(num_entities, num_relations, _hidden_dim, _margin, \"transE\")\n",
        "    if _cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # We use Adam as our optimizer, which provides an adaptive learning rate\n",
        "    # per parameter, but based on empirical testing, using a learning rate\n",
        "    # scheduler provided slightly better performance\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=_lr, weight_decay=_weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "\n",
        "    print(\"Start Training...\")\n",
        "    for epoch in trange(_num_epochs, desc=\"Train\", unit=\"Epoch\"):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch, (positive_sample, corrupted_heads, corrupted_tails, _) in enumerate(dataloader):\n",
        "            if _cuda:\n",
        "                positive_sample = positive_sample.cuda()\n",
        "                corrupted_heads = corrupted_heads.cuda()\n",
        "                corrupted_tails = corrupted_tails.cuda()\n",
        "\n",
        "            positive_sample_dist = model(positive_sample, 'positive')\n",
        "            positive_score = F.logsigmoid(positive_sample_dist)\n",
        "            positive_sample_loss = -positive_score.mean()\n",
        "\n",
        "            # Here we take the negative of the distance values because\n",
        "            # we want the distances between (head, relation) and (tail)\n",
        "            # entities for corrupted samples to be far apart. In our model,\n",
        "            # we use \"margin - distance\", so a large distance would give\n",
        "            # a negative value that we then flip to positive. After plugging\n",
        "            # into the logsigmoid, that would yield a value close to 0\n",
        "            corrupted_head_dist = model((positive_sample, corrupted_heads), 'negative-head')\n",
        "            corrupted_head_score = F.logsigmoid(-corrupted_head_dist)\n",
        "            corrupted_head_loss = -corrupted_head_score.mean()\n",
        "\n",
        "            corrupted_tail_dist = model((positive_sample, corrupted_tails), 'negative-tail')\n",
        "            corrupted_tail_score = F.logsigmoid(-corrupted_tail_dist)\n",
        "            corrupted_tail_loss = -corrupted_tail_score.mean()\n",
        "\n",
        "            # In the paper, each corrupted_head and corrupted_tail is treated as a separate example.\n",
        "            # Here, we combine them into one, so we need to weight the positive_sample loss accordingly\n",
        "            # by adding it twice\n",
        "            loss = (positive_sample_loss + corrupted_head_loss + positive_sample_loss + corrupted_tail_loss) / 4\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            #print(f\"\\nbatch: {batch}, loss: {loss}, pos_loss: {positive_sample_loss}, neg_head_loss: {corrupted_head_loss}, neg_tail_loss: {corrupted_tail_loss}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Technically having the scheduler step on evaluation loss would be\n",
        "        # preferred. However, results did not differ much. Because running\n",
        "        # eval takes significantly longer (due to increased number of negative\n",
        "        # examples), we opted to step on total train loss. When selecting the\n",
        "        # appropriate model to use for the final test metrics,\n",
        "        # we referenced the eval metrics (MRR, Hits@K)\n",
        "        scheduler.step(total_loss)\n",
        "\n",
        "        # Evaluate the model on the valid set and save model state for testing or re-loading\n",
        "        if epoch != 0 and epoch % 10 == 0:\n",
        "            eval_model(model, eval_data, num_entities, _num_negative_samples, _test_batch_size, all_data, \"eval\")\n",
        "            save_model(model, optimizer, scheduler, epoch)\n",
        "            print(f\"\\nepoch: {epoch}, avg loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "\n",
        "    # Evaludate the model on the test set\n",
        "    eval_model(model, test_data, num_entities, _num_negative_samples, _test_batch_size, all_data, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cRlj4w1lw1I",
        "outputId": "f29338a3-0d4b-452a-f229-63f20051cf07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:   0%|          | 0/40 [00:00<?, ?Epoch/s]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\eddie\\Documents\\GitHub\\Eng-Spa-Translator\\Covid Time Series\\TESTING TRANSE ONLY.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
            "\u001b[1;32mc:\\Users\\eddie\\Documents\\GitHub\\Eng-Spa-Translator\\Covid Time Series\\TESTING TRANSE ONLY.ipynb Cell 21\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     corrupted_heads \u001b[39m=\u001b[39m corrupted_heads\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     corrupted_tails \u001b[39m=\u001b[39m corrupted_tails\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m positive_sample_dist \u001b[39m=\u001b[39m model(positive_sample, \u001b[39m'\u001b[39;49m\u001b[39mpositive\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m positive_score \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlogsigmoid(positive_sample_dist)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m positive_sample_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mpositive_score\u001b[39m.\u001b[39mmean()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\eddie\\Documents\\GitHub\\Eng-Spa-Translator\\Covid Time Series\\TESTING TRANSE ONLY.ipynb Cell 21\u001b[0m in \u001b[0;36mKGEmbedding.forward\u001b[1;34m(self, sample, sample_type)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpositive\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m   \u001b[39m# sample = Tensor([batch_size, 3]) where the\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m   \u001b[39m# 3 represents head, relation, tail\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m   head \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentity_embed,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m       dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m       index\u001b[39m=\u001b[39msample[:,\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m   )\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m   relation \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mindex_select(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelation_embed,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m       dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m       index\u001b[39m=\u001b[39;49msample[:,\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m   )\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m   tail \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentity_embed,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m       dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m       index\u001b[39m=\u001b[39msample[:,\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m   )\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m   \u001b[39m# We call .unsqueeze(1) on this data so that\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m   \u001b[39m# the output will be of dimension [batch_size, 1, 2 * self.hidden_dim]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m   \u001b[39m# so that the num dimensions match the negative examples that will be of\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eddie/Documents/GitHub/Eng-Spa-Translator/Covid%20Time%20Series/TESTING%20TRANSE%20ONLY.ipynb#X26sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m   \u001b[39m# dimension [batch_size, num_neg_samples, 2 * self.hiddenIdim]\u001b[39;00m\n",
            "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpVbZYMx9qj9"
      },
      "source": [
        "**Visualization Tools**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKEplgH4l1Lu"
      },
      "outputs": [],
      "source": [
        "# Note: This was only tested on GCP VM, it will not\n",
        "# run on Google Colab without additional tinkering\n",
        "# In addition, auxiliary files are needed that preload\n",
        "# FB15k-237 \"mid\" values to their corresponding true\n",
        "# names and index values\n",
        "\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from matplotlib import cm\n",
        "from matplotlib.patches import Circle\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "np.random.seed(9654924)\n",
        "\n",
        "def load_pickle(file_name):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        p = pickle.load(f)\n",
        "    return p\n",
        "\n",
        "def load_numpy(file_name):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        n = np.load(f)\n",
        "    return n\n",
        "\n",
        "def plot_couple_dimensions(path, model, embedding_path, examples_to_use=3, dims=(2,2)):\n",
        "    \"\"\" Given a number of triples, plot their positions on a graph\n",
        "    \"\"\"\n",
        "    mid_to_index_mapping = load_pickle(os.path.join(path, \"mid_to_index_mapping.pickle\"))\n",
        "    mid_to_name_mapping = load_pickle(os.path.join(path, \"mid_to_name_mapping.pickle\"))\n",
        "    relation_to_index_mapping = load_pickle(os.path.join(path, \"relation_to_index_mapping.pickle\"))\n",
        "    used_tuples_list = load_pickle(os.path.join(path, \"used_tuples.pickle\"))\n",
        "\n",
        "    dims_to_plot = dims[0] * dims[1]\n",
        "    entity_embedding = load_numpy(embedding_path)\n",
        "    relation_embedding = load_numpy(embedding_path.replace(\"entity_embedding\", \"relation_embedding\"))\n",
        "    num_entities, num_dim = entity_embedding.shape\n",
        "\n",
        "    # Randomly select\n",
        "    idxs = np.random.choice(range(len(used_tuples_list)), examples_to_use, replace=False)\n",
        "    selected_entities = np.array(used_tuples_list)[idxs]\n",
        "\n",
        "    # All selected head tails have the same relation between them\n",
        "    relevant_relation = int(relation_to_index_mapping[selected_entities[0][1]])\n",
        "\n",
        "    heads = [head for head, _, _ in selected_entities]\n",
        "    tails = [tail for _, _, tail in selected_entities]\n",
        "    selected_entities = heads + tails\n",
        "\n",
        "    if model == 'rotatE':\n",
        "        # Split num_dim in half after accounting for imaginary dimension\n",
        "        num_dim /= 2\n",
        "        real, imag = np.split(entity_embedding, 2, axis=1)\n",
        "        variances = (np.var(real, axis=0) + np.var(imag, axis=0)) / 2\n",
        "        X, Y = real, imag\n",
        "\n",
        "    elif model == 'transE':\n",
        "        # Best to plot in at least 2D, so we select the dimensions with\n",
        "        # highest variance and assign 2 dimensions per test dimension\n",
        "        variances = np.var(entity_embedding, axis=0)\n",
        "        double_dims = np.argsort(variances)[-(dims_to_plot*2):]\n",
        "\n",
        "        X = entity_embedding\n",
        "        Y = entity_embedding\n",
        "        for ind in range(dims_to_plot):\n",
        "            Y[:, double_dims[ind]] = entity_embedding[:, double_dims[-(ind+1)]]\n",
        "        r = relation_embedding\n",
        "\n",
        "    else:\n",
        "        raise(f\"Invalid model type for plotting: {model}\")\n",
        "\n",
        "    assert len(variances) == num_dim\n",
        "    selected_dims = np.argsort(variances)[-(dims_to_plot):]\n",
        "\n",
        "    row, col = 0, 0\n",
        "    num_row, num_col = dims\n",
        "    plt.figure(figsize=(32,20))\n",
        "    fig, axis = plt.subplots(num_row, num_col)\n",
        "    for ind, dim in enumerate(selected_dims):\n",
        "        axis[row][col].axhline(0, color='black')\n",
        "        axis[row][col].axvline(0, color='black')\n",
        "        for mid in selected_entities:\n",
        "            index = int(mid_to_index_mapping[mid])\n",
        "            x = X[index][dim]\n",
        "            y = Y[index][dim]\n",
        "            axis[row][col].scatter(x, y, marker=\"o\", s=50)\n",
        "            axis[row][col].annotate(mid_to_name_mapping[mid], (x, y), fontsize=6)\n",
        "\n",
        "        # Add concentric circles to illustrate rotational nature\n",
        "        if model == 'rotatE':\n",
        "            x_low, x_high = axis[row][col].get_xlim()\n",
        "            y_low, y_high = axis[row][col].get_ylim()\n",
        "            high = max(abs(x_high), abs(y_high), abs(x_low), abs(y_low))\n",
        "\n",
        "            # Determine the interval at which to draw concentric circles\n",
        "            interval = 1\n",
        "            while high < 1:\n",
        "                high *= 10\n",
        "                interval /= 10\n",
        "\n",
        "            multiplier = 1\n",
        "            while multiplier * interval < high:\n",
        "                axis[row][col].add_patch(Circle((0, 0), multiplier * interval, color='r', fill=False, linestyle='dotted'))\n",
        "                multiplier += 1\n",
        "            axis[row][col].set_title(f'rotatE Embedding Dim {dim}', fontsize=10)\n",
        "\n",
        "        # Draw relations as vectors\n",
        "        elif model == 'transE':\n",
        "            dim_2 = double_dims[-(ind+1)]\n",
        "            for head in heads:\n",
        "                index = int(mid_to_index_mapping[head])\n",
        "                start_x = X[index][dim]\n",
        "                start_y = Y[index][dim]\n",
        "                dist_x = r[relevant_relation][dim]\n",
        "                dist_y = r[relevant_relation][dim_2]\n",
        "                axis[row][col].arrow(start_x, start_y, dist_x, dist_y)\n",
        "            axis[row][col].set_title(f'transE Embedding Dims {dim} and {dim_2}', fontsize=10)\n",
        "\n",
        "        col += 1\n",
        "        if col == num_col:\n",
        "            col = 0\n",
        "            row += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model}_embeddings')\n",
        "relational_embedding = np.load(\"modelscopy/relation_embedding_30.npy\",allow_pickle=True)\n",
        "\n",
        "entity_embedding = np.load(\"modelscopy/entity_embedding_30.npy\",allow_pickle=True)\n",
        "\n",
        "print(relational_embedding.shape)\n",
        "print(entity_embedding.shape)\n",
        "\n",
        "#plot_couple_dimensions(\"\", \"transE\", \"relation_to_index_mapping.pickle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5G0aAKCGhLZ"
      },
      "source": [
        "**MID to Entity Name Mapping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z0tpi6cGY4N"
      },
      "outputs": [],
      "source": [
        "# Similar to above, this was run on a GCP VM, so will not run\n",
        "# directly on Colab without making adjustments\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import requests\n",
        "import sys\n",
        "import tqdm\n",
        "import random\n",
        "\n",
        "# Given a relation, find all corresponding MIDs and their indexes\n",
        "relation = \"872022\"\n",
        "path = \"transDatacopy\"\n",
        "decode_file = 'fb2w.nt'\n",
        "\n",
        "entities_file = os.path.join(path, 'entities.dict')\n",
        "relations_file = os.path.join(path, 'relations.dict')\n",
        "\n",
        "mid_to_index_mapping = dict()\n",
        "mid_to_name_mapping = dict()\n",
        "mid_to_url_mapping = dict()\n",
        "\n",
        "def process_grep_out(out):\n",
        "    tuples = out.split('\\n')\n",
        "    return [tuple(tup.split('\\t')) for tup in tuples]\n",
        "\n",
        "def add_mappings(mid):\n",
        "    got_mid = True\n",
        "    if mid not in mid_to_index_mapping:\n",
        "        got_mid = False\n",
        "        stream = os.popen(f'grep {mid} {entities_file}')\n",
        "        out = stream.read()\n",
        "        entity_and_ids = process_grep_out(out)\n",
        "        for entity_and_id in entity_and_ids:\n",
        "            if len(entity_and_id) == 2:\n",
        "                eid, entity = entity_and_id\n",
        "                if entity == mid:\n",
        "                    mid_to_index_mapping[mid] = eid\n",
        "                    got_mid = True\n",
        "\n",
        "    if mid not in mid_to_url_mapping:\n",
        "        got_mid = False\n",
        "        converted_mid = mid[1:2] + '.' + mid[3:]\n",
        "        stream = os.popen(f'grep {converted_mid} {decode_file}')\n",
        "        out = stream.read()\n",
        "        mid_to_urls = process_grep_out(out)\n",
        "        for mid_to_url in mid_to_urls:\n",
        "            if len(mid_to_url) == 3:\n",
        "                fb, w3, wiki = mid_to_url\n",
        "                found_mid = fb[1:-1].split('/')[4]\n",
        "                if found_mid == converted_mid:\n",
        "                    mid_to_url_mapping[mid] = wiki[1:-3]\n",
        "                    got_mid = True\n",
        "        # If didn't find mid, then remove it from the\n",
        "        # mid_to_index_mapping map b/c it can't be used\n",
        "        if not got_mid:\n",
        "            mid_to_index_mapping.pop(mid)\n",
        "    return got_mid\n",
        "\n",
        "def generate_relation_map():\n",
        "    \"\"\" Only needs to be generated once\n",
        "    \"\"\"\n",
        "    with open(relations_file, 'r') as f:\n",
        "        all_relations = f.read()\n",
        "\n",
        "    relation_to_index_mapping = dict()\n",
        "    list_relations = all_relations.split('\\n')\n",
        "    for relation in list_relations:\n",
        "        out = relation.split('\\t')\n",
        "        if len(out) == 2:\n",
        "            rid, curr_relation = out\n",
        "            relation_to_index_mapping[curr_relation] = rid\n",
        "\n",
        "    with open(\"relation_to_index_mapping.pickle\", 'wb') as f:\n",
        "        pickle.dump(relation_to_index_mapping, f)\n",
        "\n",
        "test_file = os.path.join(path, 'test.txt')\n",
        "stream = os.popen(f'grep {relation} {test_file}')\n",
        "out = stream.read()\n",
        "tuples = process_grep_out(out)\n",
        "\n",
        "if os.path.exists('mid_to_index_mapping.pickle') and os.path.exists('mid_to_url_mapping.pickle'):\n",
        "    with open('mid_to_index_mapping.pickle', 'rb') as f:\n",
        "        mid_to_index_mapping = pickle.load(f)\n",
        "    with open('mid_to_url_mapping.pickle', 'rb') as f:\n",
        "        mid_to_url_mapping = pickle.load(f)\n",
        "else:\n",
        "    all_tuples = []\n",
        "    for _, tup in enumerate(tqdm.tqdm(tuples, desc=\"Index and URL For Tuple\")):\n",
        "        if len(tup) == 3:\n",
        "            head, relation, tail = tup\n",
        "            if add_mappings(head) and add_mappings(tail):\n",
        "                all_tuples.append(tup)\n",
        "\n",
        "    with open('mid_to_index_mapping.pickle', 'wb') as f:\n",
        "        pickle.dump(mid_to_index_mapping, f)\n",
        "    with open('mid_to_url_mapping.pickle', 'wb') as f:\n",
        "        pickle.dump(mid_to_url_mapping, f)\n",
        "    with open('used_tuples.pickle', 'wb') as f:\n",
        "        pickle.dump(all_tuples, f)\n",
        "\n",
        "for _, (mid, url) in enumerate(tqdm.tqdm(mid_to_url_mapping.items(), desc=\"Name For Tuple\")):\n",
        "    request = requests.get(url)\n",
        "    x = json.loads(request.text)['entities']\n",
        "    keys = list(x.keys())\n",
        "    name = x[keys[0]]['labels']['en']['value']\n",
        "    mid_to_name_mapping[mid] = name\n",
        "\n",
        "with open('mid_to_name_mapping.pickle', 'wb') as f:\n",
        "    pickle.dump(mid_to_name_mapping, f)\n",
        "\n",
        "generate_relation_map()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
